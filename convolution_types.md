## DILATED CONVOLUTION
Dilated convolution also known as Atrous convolution is a very useful type of convolution especially in segmentation based problems. Dilated convolutions are a way of increasing global receptive field of the network exponentially. This type of convolution helps integrating the knowledge of the wider context with lesser cost(i.e. by decreasing the number of parameters). Atrous convolutions introduce another hyper-pararmeter to the convolutional layers called the dilation rate(generally represented by l). This parameter defines the spacing between the values in a kernel. Dilated convolutions as mentioned earlier are used mainly in the real-time segmentation applications. The two main reasons for this are that it provides a wider context for better segmentation and at the same time, maintains if not reduces the number of parameters therefore, reducing computational cost making it suitable for real-time applications. Dilated convolutions can be applied in non-vision based applications as well. Text translation which requires better/more global context and real time speed uses Atrous convolutions. The gif below shows the dilated convolution.
![Dilated convolution](https://cdn-images-1.medium.com/max/600/1*SVkgHoFoiMZkjy54zM_SUw.gif)

## DEPTHWISE SEPERABLE CONVOLUTION
This is a type of convolution introduced to increase the speed of a neural network drastically and also reduce the memory used by decreasing the number of parameters. In a regular convolution, for every output channel of a layer one filter must be applied to each input channel. This is done to ensure the matrices are matching and calculations are possible. This however, severely increases the number of parameters and hence memory and time required for computation. In depthwise seperable convolutions however, we apply an arbitrary number of filters(any number generally much smaller the number of input or output channels). This therefore, splits the problem into two stages as the matrix multiplication needs to be balanced. It therefore is two steps of (Input Channel) X K X 3 X 3 (Each filter is 3X3) followed by a 1X1 convolution of the form (Input Channel) X K X 20 X 1 X 1. This can be calculated to see that although this requires two steps, reduces the number of parameters required significantly and although parallel processing of each channel independently thus increasing performance and reducing time taken for each forward and backward pass. This concept is also known as MobileNet as it makes any extremely lightweight and fast. These convolutions are therefore mostly used in any real time applications and embedded systems.
![Depthwise Seperable Convolution](https://cdn-images-1.medium.com/max/1600/1*SRBSbojkg48DTUMcP5VVHg.jpeg =500x350)


## GROUPED CONVOLUTION
Grouped convolutions also known as grouped convolutions were introduced in the famous AlexNet paper in 2012. This was introduced mainly to be able to divide the workload over multiple GPUs and speed up the training process and allow for usage of higher batch sizes due to larger memory availability. The other advantage of using grouped convolutions is that it is possible to use the different types of convolutions between the same layers and regroup them after. This allows us, say for example to use a dilated convolution and a normal convolution simultaneously, reducing the total number of parameters and increasing the receptive field with lesser loss in accuracy. Although first used in AlexNet the idea was dropped until it was shown that it infact not only helps split up the parameters but also increases accuracy because it learns different features which can be recombined better. It was then reused in ResNeXt to obtain a small accuracy boost. It also increases the variety and range of possibilities to experiment and play around with the network architecture. A simple image representing this convolution is shown below.
![Grouped Convolution](https://blog.yani.io/assets/images/posts/2017-08-10-filter-group-tutorial/convlayer.svg)
